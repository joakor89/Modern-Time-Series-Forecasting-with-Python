{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff172d0-200e-45c7-bd93-12c66a8f739a",
   "metadata": {},
   "source": [
    "# Preprocessing London Smart Meter Dataset\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8784e370-a9e3-4508-8c65-592a4dcc86dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing The Working Directory to The Root\n",
    "# %cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b80aec8c-5f93-46c6-9e78-8ebafecf32e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# OS\n",
    "import os\n",
    "import pickleshare\n",
    "\n",
    "# PyArrow\n",
    "import pyarrow as pa\n",
    "\n",
    "# Path & Notebook Optimizer\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ea4c180-0a14-4fc1-9f58-94e04e6d8d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbbfae4c-c9bf-4ff0-a5aa-6e2b7fc589ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "np.random.seed()\n",
    "\n",
    "pio.templates.default = \"plotly_white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a101a10-1b1f-4817-9d4e-df7bc7a2022c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pickleshare in /opt/anaconda3/lib/python3.12/site-packages (0.7.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pickleshare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe5ceb1-dcd0-49cd-9f5a-ec1aee5ea7af",
   "metadata": {},
   "source": [
    "### Reading Data to Pandas Dataframe\n",
    "\n",
    "#### Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdbc3f52-5776-40a2-abb2-7d497b71255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"/Users/joaquinromero/Desktop/MTSF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e811bdda-af9a-46d1-9719-b8098ddbbdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joaquinromero/Desktop/MTSF/chap_02\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "883c4582-ffaa-43ac-9e4e-6df000a0c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = Path(\"data/london_smart_meters/\")\n",
    "\n",
    "block_data_path = source_data/\"hhblock_dataset\"/\"hhblock_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed4a5f57-ba8b-4993-ab58-43792d729e3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Please check if the dataset has been downloaded properly. Refer to the Preface of the book or the Readme in the repo for expected data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m block_data_path\u001b[38;5;241m.\u001b[39mis_dir(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check if the dataset has been downloaded properly. Refer to the Preface of the book or the Readme in the repo for expected data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Please check if the dataset has been downloaded properly. Refer to the Preface of the book or the Readme in the repo for expected data"
     ]
    }
   ],
   "source": [
    "assert block_data_path.is_dir(), \"Please check if the dataset has been downloaded properly. Refer to the Preface of the book or the Readme in the repo for expected data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c144b531-9cf0-4d15-ba7e-afbd122867ab",
   "metadata": {},
   "source": [
    "#### Converting The Half-Hourly Block Level Dataset into a Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e719020-4060-48d1-a8b4-e6bb592d3b3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/london_smart_meters/hhblock_dataset/hhblock_dataset/block_0.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m block_1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock_data_path\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblock_0.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m block_1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(block_1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m], yearfirst\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m block_1\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MTSP/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MTSP/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MTSP/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MTSP/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MTSP/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/london_smart_meters/hhblock_dataset/hhblock_dataset/block_0.csv'"
     ]
    }
   ],
   "source": [
    "block_1 = pd.read_csv(block_data_path/\"block_0.csv\", parse_dates=False)\n",
    "\n",
    "block_1['day'] = pd.to_datetime(block_1['day'], yearfirst=True)\n",
    "\n",
    "block_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2216be-ba01-4ec1-9124-a73bf58db511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check End Dates of All Time Series\n",
    "block_1.groupby(\"LCLid\")['day'].max().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d18d4e-d68b-4821-ba85-f7dab3f0ba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date = None\n",
    "\n",
    "for f in tqdm(block_data_path.glob(\"*.csv\")):\n",
    "    df = pd.read_csv(f, parse_dates=False)\n",
    "    df['day'] = pd.to_datetime(df['day'], yearfirst=True)\n",
    "    if max_date is None:\n",
    "        max_date = df['day'].max()\n",
    "    else:\n",
    "        if df['day'].max()>max_date:\n",
    "            max_date = df['day'].max()\n",
    "\n",
    "print(f\"Max Date across all blocks: {max_date}\")\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf8c2f-fd73-4d07-89c3-f5d1503fa3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping The Dataframe into The Long Form with Hour Blocks along The Rows\n",
    "block_1 = block_1.set_index(['LCLid', \"day\"]).stack().reset_index().rename(columns={\"level_2\": \"hour_block\", 0: \"energy_consumption\"})\n",
    "\n",
    "# Creating a Numerical Hourblock Column\n",
    "block_1['offset'] = block_1['hour_block'].str.replace(\"hh_\", \"\").astype(int)\n",
    "\n",
    "block_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1368d3-fc42-450c-9184-da8498eb9d43",
   "metadata": {},
   "source": [
    "### Compact Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51adaba9-12f9-42ed-8f32-bf2a7e5610e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_compact(x):\n",
    "    start_date = x['day'].min()\n",
    "    name = x['LCLid'].unique()[0]\n",
    "    ### Fill missing dates with NaN ###\n",
    "    # Create a date range from  min to max\n",
    "    dr = pd.date_range(start=x['day'].min(), end=max_date, freq=\"1D\")\n",
    "    # Add hh_0 to hh_47 to columns and with some unstack magic recreating date-hh_x combinations\n",
    "    dr = pd.DataFrame(columns=[f\"hh_{i}\" for i in range(48)], index=dr).unstack().reset_index()\n",
    "    # renaming the columns\n",
    "    dr.columns = [\"hour_block\", \"day\", \"_\"]\n",
    "    # left merging the dataframe to the standard dataframe\n",
    "    # now the missing values will be left as NaN\n",
    "    dr = dr.merge(x, on=['hour_block','day'], how='left')\n",
    "    # sorting the rows\n",
    "    dr.sort_values(['day',\"offset\"], inplace=True)\n",
    "    # extracting the timeseries array\n",
    "    ts = dr['energy_consumption'].values\n",
    "    len_ts = len(ts)\n",
    "    return start_date, name, ts, len_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4375a9c0-0b0b-44ea-a1f9-59b07cbf5ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_process_block_compact(block_df, freq=\"30min\", ts_identifier=\"series_name\", value_name=\"series_value\"):\n",
    "    grps = block_df.groupby('LCLid')\n",
    "    all_series = []\n",
    "    all_start_dates = []\n",
    "    all_names = []\n",
    "    all_data = {}\n",
    "    all_len = []\n",
    "    for idx, df in tqdm(grps, leave=False):\n",
    "        start_date, name, ts, len_ts = preprocess_compact(df)\n",
    "        all_series.append(ts)\n",
    "        all_start_dates.append(start_date)\n",
    "        all_names.append(name)\n",
    "        all_len.append(len_ts)\n",
    "\n",
    "    all_data[ts_identifier] = all_names\n",
    "    all_data['start_timestamp'] = all_start_dates\n",
    "    all_data['frequency'] = freq\n",
    "    all_data[value_name] = all_series\n",
    "    all_data['series_length'] = all_len\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "block1_compact = load_process_block_compact(block_1, freq=\"30min\", ts_identifier=\"LCLid\", value_name=\"energy_consumption\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0415c0ed-3dd2-4156-a3c1-5253e1b5dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "block1_compact.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaeb118-06a7-4801-ba53-90d2d7436d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(block1_compact.memory_usage(deep=True))\n",
    "\n",
    "print(f\"Total: {block1_compact.memory_usage(deep=True).sum()/1024**2} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f7249-8e94-4b17-9181-4656e1d0be0b",
   "metadata": {},
   "source": [
    "### Expanded Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6092a429-cb60-4fa2-940e-d6d3bc7fbe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_expanded(x):\n",
    "    start_date = x['day'].min()\n",
    "    ### Fill missing dates with NaN ###\n",
    "    # Create a date range from  min to max\n",
    "    dr = pd.date_range(start=x['day'].min(), end=x['day'].max(), freq=\"1D\")\n",
    "    # Add hh_0 to hh_47 to columns and with some unstack magic recreating date-hh_x combinations\n",
    "    dr = pd.DataFrame(columns=[f\"hh_{i}\" for i in range(48)], index=dr).unstack().reset_index()\n",
    "    # renaming the columns\n",
    "    dr.columns = [\"hour_block\", \"day\", \"_\"]\n",
    "    # left merging the dataframe to the standard dataframe\n",
    "    # now the missing values will be left as NaN\n",
    "    dr = dr.merge(x, on=['hour_block','day'], how='left')\n",
    "    dr['series_length'] = len(dr)\n",
    "    return dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac54bf30-c049-476b-be00-c169d489e462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_process_block_expanded(block_df, freq=\"30min\"):\n",
    "    grps = block_df.groupby('LCLid')\n",
    "    all_series = []\n",
    "    for idx, df in tqdm(grps, leave=False):\n",
    "        ts = preprocess_expanded(df)\n",
    "        all_series.append(ts)\n",
    "\n",
    "    block_df = pd.concat(all_series)\n",
    "    # Recreate Offset because there would be null rows now\n",
    "    block_df['offset'] = block_df['hour_block'].str.replace(\"hh_\", \"\").astype(int)\n",
    "    # Creating a datetime column with the date | Will take some time because operation is not vectorized\n",
    "    block_df['timestamp'] = block_df['day'] + block_df['offset']*30*pd.offsets.Minute()\n",
    "    block_df['frequency'] = freq\n",
    "    block_df.sort_values([\"LCLid\",\"timestamp\"], inplace=True)\n",
    "    block_df.drop(columns=[\"_\", \"hour_block\", \"offset\", \"day\"], inplace=True)\n",
    "    return block_df\n",
    "#     del all_series\n",
    "block1_expanded = load_process_block_expanded(block_1, freq=\"30min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c729c3c3-11eb-45c9-be35-9341df3d9f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "block1_expanded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64defb48-0e8f-446a-a541-d6d4bf58abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(block1_expanded.memory_usage())\n",
    "print(f\"Total: {block1_expanded.memory_usage().sum()/1024**2} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda6814f-f1f1-4979-9fc5-91a19e73863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del block1_expanded, block_1, block1_compact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65cc9d-0df2-4ef1-a467-7e2c6a95a812",
   "metadata": {},
   "source": [
    "### Reading & Combining All The Block Data into A Single Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e925d1e2-6479-4af3-8b83-7e2ea465f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_df_l = []\n",
    "for file in tqdm(sorted(list(block_data_path.glob(\"*.csv\"))), desc=\"Processing Blocks..\"):\n",
    "    block_df = pd.read_csv(file, parse_dates=False)\n",
    "    block_df['day'] = pd.to_datetime(block_df['day'], yearfirst=True)\n",
    "    # Taking only from 2012-01-01\n",
    "    block_df = block_df.loc[block_df['day']>=\"2012-01-01\"]\n",
    "    #Reshaping the dataframe into the long form with hour blocks along the rows\n",
    "    block_df = block_df.set_index(['LCLid', \"day\"]).stack().reset_index().rename(columns={\"level_2\": \"hour_block\", 0: \"energy_consumption\"})\n",
    "    #Creating a numerical hourblock column\n",
    "    block_df['offset'] = block_df['hour_block'].str.replace(\"hh_\", \"\").astype(int)\n",
    "    block_df_l.append(load_process_block_compact(block_df, freq=\"30min\", ts_identifier=\"LCLid\", value_name=\"energy_consumption\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d73f13-39af-418a-9ade-6d549aab3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhblock_df = pd.concat(block_df_l)\n",
    "del block_df_l\n",
    "display(hhblock_df.memory_usage(deep=True))\n",
    "print(f\"Total: {hhblock_df.memory_usage(deep=True).sum()/1024**2} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c00b86-7996-4d7d-92e3-6ef3af70b719",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhblock_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a5f7df-a135-4785-8870-33d623c2ab00",
   "metadata": {},
   "source": [
    "### Merging Additional Information\n",
    "\n",
    "#### Household Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77122fa7-27f1-43b1-b12c-1790f9584de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "household_info = pd.read_csv(source_data/\"informations_households.csv\")\n",
    "household_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ef254-4552-4a03-8f1b-1b99cbb0bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhblock_df = hhblock_df.merge(household_info, on='LCLid', validate=\"one_to_one\")\n",
    "hhblock_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8918bc-7450-4cd7-bca1-a588e0d706f5",
   "metadata": {},
   "source": [
    "#### Weather & Bank Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f9a1d-0431-4108-9ca2-4279d28ec50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_holidays = pd.read_csv(source_data/\"uk_bank_holidays.csv\", parse_dates=False)\n",
    "bank_holidays['Bank holidays'] = pd.to_datetime(bank_holidays['Bank holidays'], yearfirst=True)\n",
    "bank_holidays.set_index(\"Bank holidays\", inplace=True)\n",
    "bank_holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d0aa5-f93b-433f-ab73-493e97255d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex on Standard Date Range\n",
    "bank_holidays = bank_holidays.resample(\"30min\").asfreq()\n",
    "bank_holidays = bank_holidays.groupby(bank_holidays.index.date).ffill().fillna(\"NO_HOLIDAY\")\n",
    "bank_holidays.index.name=\"datetime\"\n",
    "bank_holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26f92d3-12dc-41be-886d-04c96e3da47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_hourly = pd.read_csv(source_data/\"weather_hourly_darksky.csv\", parse_dates=False)\n",
    "weather_hourly['time'] = pd.to_datetime(weather_hourly['time'], yearfirst=True)\n",
    "weather_hourly.set_index(\"time\", inplace=True)\n",
    "weather_hourly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dc879a-06b5-437e-9c09-80b5d55dad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resampling at 30min and forward fill\n",
    "weather_hourly = weather_hourly.resample(\"30min\").ffill()\n",
    "weather_hourly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996dcbd3-bb50-4168-9d3a-22b4e7200046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_weather_holidays(row):\n",
    "    date_range = pd.date_range(row['start_timestamp'], periods=row['series_length'], freq=row['frequency'])\n",
    "    std_df = pd.DataFrame(index=date_range)\n",
    "    #Filling Na iwth NO_HOLIDAY cause rows before earliers holiday will be NaN\n",
    "    holidays = std_df.join(bank_holidays, how=\"left\").fillna(\"NO_HOLIDAY\")\n",
    "    weather = std_df.join(weather_hourly, how='left')\n",
    "    assert len(holidays)==row['series_length'], \"Length of holidays should be same as series length\"\n",
    "    assert len(weather)==row['series_length'], \"Length of weather should be same as series length\"\n",
    "    row['holidays'] = holidays['Type'].values\n",
    "    for col in weather:\n",
    "        row[col] = weather[col].values\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a1db1-26ce-4edd-87d6-1619aebde7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhblock_df = hhblock_df.progress_apply(map_weather_holidays, axis=1)\n",
    "\n",
    "hhblock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de460c63-ee9c-49ff-b38c-595f54b96e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del block_df, weather_hourly, bank_holidays, household_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a0140-9124-48ad-8247-1661c75679a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(hhblock_df.memory_usage(deep=True))\n",
    "print(f\"Total: {hhblock_df.memory_usage(deep=True).sum()/1024**2} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708b20e5-6af3-4fde-ba06-575a4db287ff",
   "metadata": {},
   "source": [
    "#### Saving The File on Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546bf60b-1cd3-4032-9c19-fe71e78c1872",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data/london_smart_meters/preprocessed\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f94c26-49a3-487c-908e-775b4cf971c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a long time to finish. Comment out and execute only if needed\n",
    "from src.utils.data_utils import write_compact_to_ts\n",
    "write_compact_to_ts(hhblock_df,\n",
    "       static_columns = ['LCLid', 'start_timestamp', 'frequency','series_length', 'stdorToU', 'Acorn', 'Acorn_grouped', 'file'], \n",
    "       time_varying_columns = ['energy_consumption', 'holidays', 'visibility', 'windBearing', 'temperature', 'dewPoint',\n",
    "                              'pressure', 'apparentTemperature', 'windSpeed', 'precipType', 'icon','humidity', 'summary'],\n",
    "       filename=f\"data/london_smart_meters/preprocessed/london_smart_meters_merged.ts\",\n",
    "       sep=\";\",\n",
    "      chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b571ed6-f14c-4f23-bc0d-89636dbb5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the LCLid - Acorn map as a pickle to be used later\n",
    "hhblock_df[['LCLid',\"file\", \"Acorn_grouped\"]].to_pickle(f\"data/london_smart_meters/preprocessed/london_smart_meters_lclid_acorn_map.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b4608-543d-4852-ba62-1739757b7a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the blocks into 8 chunks\n",
    "blocks = [f\"block_{i}\" for i in range(111)]\n",
    "\n",
    "n_chunks= 8\n",
    "split_blocks = [blocks[i:i + n_chunks] for i in range(0, len(blocks), n_chunks)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254c7172-1188-4056-8b39-8c664b531a18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Writing each chunk to disk\n",
    "for blk in tqdm(split_blocks):\n",
    "    df = hhblock_df.loc[hhblock_df.file.isin(blk)]\n",
    "    blk = [int(b.replace(\"block_\",\"\")) for b in blk]\n",
    "    block_str = f\"block_{min(blk)}-{max(blk)}\"\n",
    "    df.to_parquet(f\"data/london_smart_meters/preprocessed/london_smart_meters_merged_{block_str}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed7c6d1-7dee-4d3b-8c52-c7c9ea5a0052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be8952-8364-4cae-ba0b-3ede7c23aa6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
